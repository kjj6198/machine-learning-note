### 還沒有整理的雜亂筆記

機器學習筆記
Week 1 - Linear Regression
尚未整理

多數的深度學習傾向於尋找最小的損失函數，因為用交叉驗證的方式來驗證你的演算法往往需要很多時間。

隨機梯度下降法是個很好尋找最小損失函數的方式。

樣本的損失和導數 S.G.D

輸入：零均值、同方差

降低學習率

adagrad

設計一個最好的損失函數。

Softmax

我們想要讓 output label 的機率總和為 1，最傳統的方法是直接求算平均後，每個 label 所佔的權重。這裡會有問題，如果 output 的 label 值差別不大，機器就很難分辨應該要選哪個。

所以我們想要的是，讓大的 label 盡可能獲得更高的權重，小的 label 則讓他們趨近於 0。有一個神奇的公式：

Cross Entropy

$$ D(S, L) = -\Sigma Li\log(Si) $$

用 cross entropy 來嵌入，來讓相對應的 label 只有 0,1 的輸出。

可以看出，entropy 其實是信息量的期望值。值越大，代表這個數值的不確定性越高，相反地，值越小代表我們越確定。我們想要這個值盡可能的小。

梯度下降法 gradient descent

對每個懲罰函數每一個變量求偏導數，並將每個變量值加上改偏導數值，直到找到最小值。

初始化

標準化數據。讓平均值為零
找一個小的 sigma
邏輯回歸

我們使用邏輯回歸分類器，但是發現這個 model 實在慢得要命，就算 model 已經很小了。這很正常ㄌ

所以為了加速 model 的訓練效率，我們必須找出更好的方式來找到最小損失函數。

隨機梯度下降法

因為梯度下降法的成本很高。

隨機梯度下降法的精髓在於隨機，每次的步伐都盡量的小，儘管步數會變多，但是比起效能的犧牲，兩者還是有足夠明顯的差距。

我們估計損失值，損失值的方式有點隨機，是用某部分的樣本來計算的。讓每一步的損失值變得小但是步數變得多。

越接近目標，就走越小的步伐。

動量 momentum

將圖像轉成二維陣列

驗證 normalize 的圖像

維持數據平衡

樣本亂序

邏輯回歸分類器

如何完全通過優化器計算任意函數的斜率？

線性模型的局限性在於...他是線性的。這樣說有點蠢，但是當你的輸入關係並非線性關係時，強大的線性模型就派不上用場了。

線性模型有幾個好處：

快，非常容易操作
用大量的矩陣來表示，這正是 GPU 的專長
誤差值小，輸入跟輸出不會造成太大的波動
所以我們想要的是：

輸入的參數盡可能地多
輸入的參數關係可以是非線性的
RELU

邏輯分類器 ====> 經過修改 ======> 非線性函數的模型

兩層神經網路

第一層由一組 X 的權重和偏差 mx + b 組成，並通過 ReLU。這一層的輸出會提供給下一層。但是在神經網路外部不可見，所以叫做 hidden layer
第二層由隱藏層的 mx + b 組成，然後用 softmax 生成。
反向傳播

該方法使得計算函數偏導數的過程變得十分高效。

Regularization

在模型中加入懲罰函數。

Dropout

卷積神經網路 convolution

權重共享

通過改變它的結構來優化整個模型，比如他有多少層，他們是怎麼連接在一起的，間格步數、池化方法等等。嘗試用 dropout 跟正則話來進行更有效率的訓練。

Average pooling

1x1 convolutions

特徵圖尺寸
max pooling
LENET-5
ALEXNET
文本和序列的深度模型

難題：想要知道這個詞的意思，又想要知道這個詞跟其他詞的關聯

word2vec / CBOW

接下來，我們想要尋找離我們相近的文本。有兩個方法：

尋找相鄰
降維
PCA 轉換過程丟失太多訊息了
t-SNE 投影 ===> 原本距離很遠的，轉換後也要互相遠離
RNNs

遞歸神經網路(RNN)是近年來最蓬勃發展的深度學習網路架構，在架構上跟傳統的類神經網路有很大的不同。遞歸神經網路的神經元內有一個暫存的記憶空間，可以把先前輸入資料產生的狀態儲存在暫存的記憶空間(internal memory)內，之後神經元就可以根據之前的狀態而計算出不同的輸出值。因為遞歸神經網路可以儲存先前的狀態，所以可以處理不同長度的輸入資料，對時間序列(Time series)、自然語言處理(Nature language processing)、語音辨識等應用有非常好的效果。 雖然遞歸神經網路是如此強大，但在實務的訓練上卻有一些問題。權重組合的空間形狀對隨機梯度下降法很不利，有很平緩的地方也有非常陡峭的山谷。平緩的地方會有梯度消失(Vanishing gradient)的問題，會讓隨機梯度下降法停留在局部最佳解，而非常陡峭的山谷容易讓隨機梯度下降法更新後的數值跑出正常的範圍，使得隨機梯度下降法產生很不穩定的結果。

根據事件序列在不同的時間點做出不同的預測

====x1====x2===x3

====t1====t2====t3

用單個模型總結過去的訊息。

反向傳播時間

梯度爆炸 / 消失

LSTM(Long Short-Term Memory) 長短期記憶

將離散的值用連續的 function 替換，就可以對他求導數。===> 反向傳播

用 tan 將其限制在 -1 ~ 1 之間。

          Y
past  |--------|  future
>>>>>>|   W    | >>>>>
      |--------|
      X
長短期記憶神經網路(Long-short term memory, LSTM)跟遞歸神經網路最大的不同，就是在神經元中加入了三個控制用的開關(Gate)，分別是寫入(input)、遺忘(forget)、輸出(output)。這三個開關有各自的權重，會依據輸入資料經過權重計算之後來決定每個開關的開啟或關閉。寫入開關用來控制資料是否寫入內部記憶空間、遺忘開關用來控制是否把先前記憶空間中的內容保留、輸出開關用來控制記憶空間中的數值是否要輸出。雖然增加了這些開關而有更多的權重需要搜尋，但有了這三個開關就能減少遞歸神經網路在使用隨機梯度下降法時碰到的問題。目前常見深度學習中用到遞歸神經網路架構時，大多會使用長短期記憶神經網路或他的簡化版本GRU(Gated Recurrent Unit)。

Bean 搜尋

只採用最合適的，剩下砍掉。

RNNs 生成浚洌，預測字母或單字，根據機率挑選合適的樣本。

就像在玩積木。